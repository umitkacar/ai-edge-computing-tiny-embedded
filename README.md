<div align="center">

# ğŸš€ AI Edge Computing & TinyML
### *Comprehensive Guide to State-of-the-Art Edge AI*

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=32&duration=2800&pause=1000&color=00D9FF&center=true&vCenter=true&width=940&lines=AI+Edge+Computing+%26+TinyML;Ultra-Low+Power+AI+Systems;Deploy+AI+on+Embedded+Devices;Real-Time+Inference+at+the+Edge" alt="Typing SVG" />

[![GitHub stars](https://img.shields.io/github/stars/umitkacar/ai-edge-computing-tiny-embedded?style=for-the-badge&logo=github&color=yellow)](https://github.com/umitkacar/ai-edge-computing-tiny-embedded/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/umitkacar/ai-edge-computing-tiny-embedded?style=for-the-badge&logo=github&color=blue)](https://github.com/umitkacar/ai-edge-computing-tiny-embedded/network/members)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge&logo=opensourceinitiative)](LICENSE)
[![Latest Update](https://img.shields.io/badge/Updated-January_2025-ff69b4?style=for-the-badge&logo=clockify)](https://github.com/umitkacar/ai-edge-computing-tiny-embedded)

---

### ğŸŒŸ **Latest Update: January 2025**
> **Production-Ready Python Implementation** with modern tooling (Hatch, Ruff, Mypy)
> **62/62 Tests Passing** â€¢ **81.76% Coverage** â€¢ **Zero Security Issues**
> **State-of-the-Art Algorithms & Trends** for Edge AI and Embedded Systems

</div>

---

## ğŸ“‹ **Table of Contents**

<table>
<tr>
<td width="33%" valign="top">

### ğŸš€ **Getting Started**
- [ğŸ“¦ Installation](#-installation)
- [ğŸ› ï¸ Development Setup](#%EF%B8%8F-development-setup)
- [ğŸ“Š Project Structure](#-project-structure)
- [âœ… Quality Assurance](#-quality-assurance)
- [ğŸ¯ Features & Examples](#-features)

</td>
<td width="33%" valign="top">

### ğŸ”¥ **Core Topics**
- [ğŸ¯ SOTA Models 2024-2025](#-sota-models--algorithms-2024-2025)
- [ğŸ‘ï¸ Object Detection](#-object-detection-models)
- [ğŸ¤– Small Language Models](#-small-language-models-slms-for-edge)
- [âš¡ State Space Models](#-state-space-models---efficient-transformers)

</td>
<td width="33%" valign="top">

### ğŸ› ï¸ **Frameworks & Tools**
- [ğŸš€ Inference Frameworks](#-inference-frameworks--runtimes)
- [ğŸ”§ Model Optimization](#-model-compression--optimization)
- [ğŸ’» Hardware Platforms](#-hardware-acceleration--platforms)
- [ğŸŒ Deployment Tools](#-edge-deployment-frameworks)

</td>
</tr>
<tr>
<td width="33%" valign="top">

### ğŸ“š **Documentation**
- [ğŸ“„ CHANGELOG.md](CHANGELOG.md)
- [ğŸ“š LESSONS-LEARNED.md](LESSONS-LEARNED.md)
- [ğŸ”§ DEVELOPMENT.md](DEVELOPMENT.md)

</td>
<td width="33%" valign="top">

### ğŸ“š **Resources**
- [ğŸ¯ TinyML & MCU](#-tinyml--mcu-specific-advances)
- [âš™ï¸ Compilers](#%EF%B8%8F-compilers--low-level-frameworks)
- [ğŸ“„ Research Papers](#-research-papers--academic-resources)

</td>
<td width="33%" valign="top">

### ğŸ“ **Community**
- [ğŸ¤ Contributing](#-contributing--community)
- [ğŸ“Š Repository Stats](#-repository-stats)
- [ğŸ·ï¸ Keywords](#%EF%B8%8F-keywords)

</td>
</tr>
</table>

---

<div align="center">

## ğŸš€ **Quick Start & Development**

![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Hatch](https://img.shields.io/badge/Hatch-Build_System-4051B5?style=for-the-badge&logo=pypi&logoColor=white)
![Tests](https://img.shields.io/badge/Tests-62%2F62_Passing-success?style=for-the-badge&logo=pytest&logoColor=white)
![Coverage](https://img.shields.io/badge/Coverage-81.76%25-brightgreen?style=for-the-badge&logo=codecov&logoColor=white)
![Type](https://img.shields.io/badge/Type_Checked-Mypy_Strict-blue?style=for-the-badge&logo=python&logoColor=white)

</div>

### ğŸ“¦ **Installation**

This project uses modern Python tooling with [Hatch](https://hatch.pypa.io/) for dependency management and development workflows.

```bash
# Clone the repository
git clone https://github.com/umitkacar/ai-edge-computing-tiny-embedded.git
cd ai-edge-computing-tiny-embedded

# Install dependencies (using hatch)
pip install hatch

# Run tests
hatch run test

# Run full CI pipeline
hatch run ci
```

### ğŸ› ï¸ **Development Setup**

**Modern Python Stack:**
- **Build System:** [Hatch](https://hatch.pypa.io/) - Modern Python project manager
- **Linting:** [Ruff](https://docs.astral.sh/ruff/) - Ultra-fast Python linter (100x faster than flake8)
- **Formatting:** [Black](https://black.readthedocs.io/) - The uncompromising code formatter
- **Type Checking:** [Mypy](https://mypy.readthedocs.io/) - Static type checker (strict mode)
- **Testing:** [Pytest](https://docs.pytest.org/) - Comprehensive test framework
- **Security:** [Bandit](https://bandit.readthedocs.io/) - Security vulnerability scanner
- **Pre-commit:** Automated quality checks on commit/push

**Available Commands:**

```bash
# Linting & Formatting
hatch run lint          # Run Ruff linter
hatch run format        # Format code with Black
hatch run format-check  # Check formatting without changes

# Type Checking
hatch run type-check    # Run Mypy strict type checking

# Testing
hatch run test                    # Run tests (sequential)
hatch run test-parallel           # Run tests with auto workers
hatch run test-parallel-cov       # Parallel tests with coverage

# Security
hatch run security      # Run Bandit security audit

# Complete CI Pipeline
hatch run ci           # Run all checks (format, lint, type-check, security, test)
```

### ğŸ“Š **Project Structure**

```
ai-edge-computing-tiny-embedded/
â”œâ”€â”€ src/ai_edge_tinyml/          # Source code (src layout)
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ quantization.py          # INT8/INT4/FP16 quantization
â”‚   â”œâ”€â”€ model_optimizer.py       # Model optimization pipeline
â”‚   â”œâ”€â”€ utils.py                 # Utility functions
â”‚   â””â”€â”€ py.typed                 # PEP 561 marker (typed package)
â”œâ”€â”€ tests/                       # Test suite (62 tests, 81.76% coverage)
â”‚   â”œâ”€â”€ conftest.py              # Pytest configuration & fixtures
â”‚   â”œâ”€â”€ test_quantization.py     # Quantization tests (21 tests)
â”‚   â”œâ”€â”€ test_model_optimizer.py  # Optimizer tests (19 tests)
â”‚   â””â”€â”€ test_utils.py            # Utility tests (22 tests)
â”œâ”€â”€ pyproject.toml               # Project configuration (single source of truth)
â”œâ”€â”€ .pre-commit-config.yaml      # Pre-commit hooks configuration
â”œâ”€â”€ CHANGELOG.md                 # Detailed change history
â”œâ”€â”€ LESSONS-LEARNED.md           # Best practices & insights
â”œâ”€â”€ DEVELOPMENT.md               # Development guidelines
â””â”€â”€ README.md                    # This file
```

### âœ… **Quality Assurance**

This project maintains production-ready code quality:

| Check | Status | Details |
|-------|--------|---------|
| **Ruff Linting** | âœ… PASS | 50+ rules, zero errors |
| **Black Formatting** | âœ… PASS | Line length: 100 |
| **Mypy Type Check** | âœ… PASS | Strict mode enabled |
| **Bandit Security** | âœ… PASS | 0 vulnerabilities |
| **Test Suite** | âœ… PASS | 62/62 tests passing |
| **Code Coverage** | âœ… PASS | 81.76% (exceeds 80%) |
| **Pre-commit Hooks** | âœ… PASS | 15+ automated checks |

**Test Results:**
```
tests/test_quantization.py      21 passed
tests/test_model_optimizer.py   19 passed
tests/test_utils.py             22 passed
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total: 62 passed in 0.50s âœ…
Coverage: 81.76% (exceeds 80% threshold) âœ…
```

### ğŸ”’ **Security**

- **Bandit Security Audit:** Zero vulnerabilities detected
- **Type Safety:** Full type annotations with mypy strict mode
- **Dependency Scanning:** Automated security checks in CI
- **Pre-commit Hooks:** Security validations before commit

### ğŸ“š **Documentation**

- **[CHANGELOG.md](CHANGELOG.md)** - Detailed version history and changes
- **[LESSONS-LEARNED.md](LESSONS-LEARNED.md)** - Best practices, insights, and technical decisions
- **[DEVELOPMENT.md](DEVELOPMENT.md)** - Comprehensive development guidelines
- **API Documentation:** Auto-generated from Google-style docstrings

### ğŸ¯ **Features**

**Quantization Support:**
- âœ… INT8 Quantization (8-bit integers)
- âœ… INT4 Quantization (4-bit integers)
- âœ… FP16 Quantization (16-bit floats)
- âœ… Dynamic Quantization
- âœ… Symmetric & Asymmetric modes
- âœ… Per-tensor & per-channel quantization

**Model Optimization:**
- âœ… Weight quantization with 6 different modes
- âœ… Compression ratio analysis
- âœ… Model size calculation
- âœ… Type-safe APIs with full annotations
- âœ… Comprehensive error handling

**Example Usage:**

```python
import numpy as np
from ai_edge_tinyml import Quantizer, QuantizationConfig, QuantizationMode

# Create quantization config
config = QuantizationConfig(
    mode=QuantizationMode.INT8,
    symmetric=True,
    per_channel=False
)

# Initialize quantizer
quantizer = Quantizer(config)

# Quantize weights
weights = np.random.randn(100, 100).astype(np.float32)
quantized = quantizer.quantize(weights)

# Dequantize for inference
dequantized = quantizer.dequantize(quantized)

# Calculate compression
from ai_edge_tinyml.utils import calculate_compression_ratio
ratio = calculate_compression_ratio(weights, quantized)
print(f"Compression ratio: {ratio:.2f}x")
```

---

<div align="center">

## ğŸ”¥ **SOTA Models & Algorithms (2024-2025)**

![AI Edge](https://img.shields.io/badge/AI-Edge_Computing-00D9FF?style=for-the-badge&logo=tensorflow&logoColor=white)
![TinyML](https://img.shields.io/badge/TinyML-Embedded_AI-FF6B6B?style=for-the-badge&logo=arduino&logoColor=white)
![SOTA](https://img.shields.io/badge/SOTA-2024--2025-4ECDC4?style=for-the-badge&logo=artifacthub&logoColor=white)

</div>

---

### ğŸ¯ **Object Detection Models**

<table>
<tr>
<td width="50%">

#### ğŸ¥‡ **YOLOv11 (YOLO11)**
![Release](https://img.shields.io/badge/Release-November_2024-brightgreen?style=flat-square&logo=github)
![Status](https://img.shields.io/badge/Status-SOTA-gold?style=flat-square&logo=hackthebox)

> ğŸš€ State-of-the-art real-time object detection with transformer-based improvements

**âœ¨ Key Features:**
- âš¡ Transformer-based backbone with C3k2 blocks
- ğŸ¯ Partial Self-Attention (PSA) mechanism
- ğŸ”¥ NMS-free training with dual label assignment
- ğŸ“‰ **25-40% lower latency** vs YOLOv10
- ğŸ“Š **10-15% improvement** in mAP
- âš¡ **60+ FPS** processing capability

**ğŸ“š Resources:**
```bash
ğŸ“– Ultralytics Docs â†’ https://docs.ultralytics.com/models/
ğŸ“„ YOLO Evolution â†’ https://arxiv.org/html/2510.09653v2
```

</td>
<td width="50%">

#### ğŸ¥ˆ **YOLOv10**
![Release](https://img.shields.io/badge/Release-May_2024-blue?style=flat-square&logo=github)
![NMS](https://img.shields.io/badge/NMS-Free-orange?style=flat-square&logo=lightning)

> âš¡ Eliminates NMS for end-to-end real-time detection

**ğŸ“Š Performance Metrics:**
- ğŸ”¸ **YOLOv10s**: 1.8x faster than RT-DETR-R18
- ğŸ”¸ **YOLOv10b**: 46% less latency, 25% fewer parameters
- ğŸ”¸ **mAP Range**: 38.5 - 54.4

**ğŸ“š Resources:**
```bash
ğŸ“„ Paper â†’ https://arxiv.org/pdf/2405.14458
ğŸ“– Docs â†’ https://docs.ultralytics.com/models/yolov10/
```

</td>
</tr>
</table>

---

#### ğŸ¤– **RT-DETR & RT-DETRv2**
![Transformer](https://img.shields.io/badge/Architecture-Transformer-blueviolet?style=for-the-badge&logo=pytorch)
![Real-Time](https://img.shields.io/badge/Real--Time-Detection-success?style=for-the-badge&logo=speedtest)

> ğŸ¯ First practical real-time detection transformer

| Model | AP Score | FPS | Device |
|-------|----------|-----|--------|
| RT-DETR | **53.1%** | 108 | NVIDIA T4 |
| RT-DETRv2 | **>55%** | 108+ | NVIDIA T4 |

**ğŸ”— Resources:**
- ğŸ“Š [RT-DETR vs YOLO11 Comparison](https://docs.ultralytics.com/compare/rtdetr-vs-yolo11/)

---

### ğŸ“± **Efficient Vision Models for Edge**

<div align="center">

```mermaid
graph LR
    A[ğŸ–¼ï¸ Input Image] --> B[ğŸ“± MobileNetV4]
    A --> C[âš¡ EfficientViT]
    B --> D[ğŸ¯ 87% Accuracy]
    C --> E[ğŸ”¥ 3.8ms Latency]
    D --> F[ğŸ“² Edge TPU]
    E --> F
    style A fill:#e1f5ff
    style B fill:#ffe1f5
    style C fill:#f5ffe1
    style D fill:#ffe1e1
    style E fill:#e1ffe1
    style F fill:#ffd700
```

</div>

---

<table>
<tr>
<td width="50%" valign="top">

#### ğŸ“± **MobileNetV4**
![ECCV](https://img.shields.io/badge/ECCV-2024-red?style=flat-square&logo=adobeacrobatreader)
![Mobile](https://img.shields.io/badge/Platform-Mobile-blue?style=flat-square&logo=android)

> ğŸŒ Universal efficient architecture for mobile ecosystem

**ğŸ¨ Innovations:**
- ğŸ”¹ Universal Inverted Bottleneck (UIB) block
- âš¡ Mobile MQA attention (**39% speedup**)
- ğŸ¯ Optimized NAS recipe
- ğŸ† **87% ImageNet accuracy** @ 3.8ms (Pixel 8 EdgeTPU)

**ğŸ“š Resources:**
- ğŸ“„ [MobileNetV4 Paper (Springer)](https://link.springer.com/chapter/10.1007/978-3-031-73661-2_5)
- ğŸ”¬ [Google Research](https://syncedreview.com/2024/04/18/87-imagenet-accuracy-3-8ms-latency-googles-mobilenetv4-redefines-on-device-mobile-vision/)

</td>
<td width="50%" valign="top">

#### âš¡ **EfficientViT**
![ViT](https://img.shields.io/badge/Type-Vision_Transformer-purple?style=flat-square&logo=lightning)
![2024](https://img.shields.io/badge/Year-2024-green?style=flat-square)

> ğŸ§  Lightweight multi-scale attention for high-resolution tasks

**âœ¨ Features:**
- ğŸ”¸ Memory-efficient Vision Transformer
- ğŸ”¸ Cascaded group attention
- ğŸ”¸ Dense prediction tasks optimized
- ğŸ”¸ High-resolution image processing

</td>
</tr>
</table>

---

<div align="center">

## ğŸ¤– **Small Language Models (SLMs) for Edge**

![LLM](https://img.shields.io/badge/Small_Language-Models-FF6B6B?style=for-the-badge&logo=openai&logoColor=white)
![Edge](https://img.shields.io/badge/Edge-Deployment-4ECDC4?style=for-the-badge&logo=raspberrypi&logoColor=white)

</div>

---

<table>
<tr>
<td width="50%">

### ğŸ§  **Microsoft Phi-3**
![Microsoft](https://img.shields.io/badge/Microsoft-Phi--3-0078D4?style=for-the-badge&logo=microsoft)

**ğŸ“Š Variants:**
```yaml
Model: Phi-3-mini
Parameters: 3.8B
Context: Up to 128K tokens
Deployment: GPU, CPU, Mobile
Status: âœ… Production Ready
```

**ğŸ¯ Optimized For:**
- ğŸ’» GPU acceleration
- ğŸ–¥ï¸ CPU inference
- ğŸ“± Mobile deployment

**ğŸ”— Resources:**
- [Phi-3 Overview](https://datasciencedojo.com/blog/small-language-models-phi-3/)

</td>
<td width="50%">

### ğŸ¦™ **TinyLlama**
![TinyLlama](https://img.shields.io/badge/TinyLlama-1.1B-orange?style=for-the-badge&logo=meta)

**ğŸ“Š Specifications:**
```yaml
Parameters: 1.1B
Target: Mobile/Edge devices
Performance: High for size class
Year: 2024
Status: âœ… Active
```

**âœ¨ Highlights:**
- ğŸ”¸ Compact architecture
- ğŸ”¸ Edge-optimized
- ğŸ”¸ Strong performance/size ratio

</td>
</tr>
<tr>
<td width="50%">

### ğŸŒŸ **Google Gemini Nano**
![Google](https://img.shields.io/badge/Google-Gemini_Nano-4285F4?style=for-the-badge&logo=google)

**ğŸ“± On-device AI for Smartphones**

**Variants:**
- ğŸ“Š **1.8B** parameters (lightweight)
- ğŸ“Š **3.25B** parameters (standard)

**ğŸ¯ Capabilities:**
- âœ… Context-aware reasoning
- âœ… Real-time translation
- âœ… Text summarization
- âœ… Edge-optimized for phones/IoT

</td>
<td width="50%">

### ğŸ¦™ **Meta Llama 3.2**
![Meta](https://img.shields.io/badge/Meta-Llama_3.2-0668E1?style=for-the-badge&logo=meta)

**ğŸ–¼ï¸ Edge AI & Vision Capabilities**

**Features:**
- âš¡ Edge deployment optimized
- ğŸ‘ï¸ Vision-language capabilities
- ğŸ“± Mobile-friendly variants
- ğŸ”¥ Latest architecture

**ğŸ”— Resources:**
- [Llama 3.2 Announcement](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)

</td>
</tr>
</table>

---

### ğŸ“· **MobileVLM**
![VLM](https://img.shields.io/badge/Vision--Language-Model-success?style=for-the-badge&logo=youtube)

> ğŸ¨ Efficient vision-language model for mobile devices

**Specifications:**
- ğŸ”¹ **mobileLLaMA**: 2.7B parameters
- ğŸ”¹ Trained from scratch on open datasets
- ğŸ”¹ Fully optimized for mobile deployment
- ğŸ”¹ Vision + Language capabilities

---

<div align="center">

## âš¡ **State Space Models - Efficient Transformers**

![SSM](https://img.shields.io/badge/State_Space-Models-blueviolet?style=for-the-badge&logo=lightning&logoColor=white)
![Efficiency](https://img.shields.io/badge/5x-Faster_Than_Transformers-gold?style=for-the-badge&logo=speedtest)

</div>

---

<table>
<tr>
<td width="50%">

### ğŸ **Mamba**
![Mamba](https://img.shields.io/badge/Mamba-SSM-green?style=for-the-badge&logo=python)

> âš¡ Linear-time sequence modeling with selective state spaces

**ğŸš€ Performance Highlights:**

| Metric | Performance |
|--------|-------------|
| Throughput | **5x higher** than Transformers |
| Scaling | **Linear** in sequence length |
| Comparison | Mamba-3B > Transformers (same size) |
| Power | Matches Transformers 2x its size |

**ğŸ“Š Advantages:**
```diff
+ âœ… Linear time complexity
+ âœ… 5x throughput improvement
+ âœ… Efficient long sequences
+ âœ… Lower memory footprint
- âŒ Newer architecture (less tested)
```

**ğŸ“š Resources:**
- ğŸ“„ [Mamba Paper](https://arxiv.org/abs/2312.00752)
- ğŸ’» [Mamba GitHub](https://github.com/state-spaces/mamba)
- ğŸ“– [Mamba Survey](https://arxiv.org/html/2408.01129v1)

</td>
<td width="50%">

### ğŸ“± **eMamba**
![eMamba](https://img.shields.io/badge/eMamba-Edge_Optimized-orange?style=for-the-badge&logo=raspberry-pi)

> ğŸ”§ Edge-optimized Mamba acceleration framework

**âœ¨ Features:**
```yaml
Design: End-to-end hardware acceleration
Target: Edge platforms
Complexity: Linear time
Status: 2024 Release
```

**ğŸ¯ Optimizations:**
- ğŸ”¹ Hardware-aware design
- ğŸ”¹ Edge platform specific
- ğŸ”¹ Leverages linear complexity
- ğŸ”¹ Memory efficient

**ğŸ“š Resources:**
- ğŸ“„ [eMamba Paper](https://arxiv.org/html/2508.10370)

</td>
</tr>
</table>

---

<div align="center">

## ğŸš€ **Inference Frameworks & Runtimes**

![Inference](https://img.shields.io/badge/High_Performance-Inference-FF6B6B?style=for-the-badge&logo=nvidia&logoColor=white)
![Runtime](https://img.shields.io/badge/Runtime-Optimization-00D9FF?style=for-the-badge&logo=apache&logoColor=white)

</div>

---

<table>
<tr>
<td width="50%">

### âš¡ **TensorRT-LLM**
![NVIDIA](https://img.shields.io/badge/NVIDIA-TensorRT--LLM-76B900?style=for-the-badge&logo=nvidia)

> ğŸ† High-performance LLM inference on NVIDIA GPUs

**ğŸ“Š Performance:**
```diff
+ 70% faster than llama.cpp on RTX 4090
+ State-of-the-art optimizations
+ Quality maintained across precisions
```

**âœ¨ Features:**
- ğŸ”¸ Python & C++ API
- ğŸ”¸ Multi-precision support
- ğŸ”¸ Advanced kernel optimization
- ğŸ”¸ Production-grade quality

**ğŸ”— Resources:**
- [TensorRT-LLM GitHub](https://github.com/NVIDIA/TensorRT-LLM)
- [Deployment Guide](https://towardsdatascience.com/deploying-llms-into-production-using-tensorrt-llm-ed36e620dac4/)

</td>
<td width="50%">

### ğŸ“„ **vLLM**
![vLLM](https://img.shields.io/badge/UC_Berkeley-vLLM-003262?style=for-the-badge&logo=databricks)

> ğŸ’¡ High-throughput LLM serving with PagedAttention

**ğŸ¯ Innovations:**
- âš¡ PagedAttention memory management
- ğŸ”¸ Optimized KV cache handling
- ğŸŒ Multi-platform support

**ğŸ–¥ï¸ Supported Hardware:**
```yaml
AMD: GPU support
Google: TPU support
AWS: Inferentia support
Base: PyTorch
```

**ğŸ”— Resources:**
- [vLLM vs TensorRT-LLM](https://northflank.com/blog/vllm-vs-tensorrt-llm-and-how-to-run-them)

</td>
</tr>
<tr>
<td width="50%">

### ğŸ¦™ **ExecuTorch**
![Meta](https://img.shields.io/badge/Meta-ExecuTorch-0668E1?style=for-the-badge&logo=meta)

> ğŸ“± Efficient LLM execution on edge devices

**Features:**
- ğŸ”¹ Lightweight edge runtime
- ğŸ”¹ Static memory planning
- ğŸ”¹ Multi-platform support
- ğŸ”¹ TorchAO quantization

**ğŸ’» Hardware Support:**
- âœ… CPU
- âœ… GPU
- âœ… AI Accelerators
- âœ… Mobile devices

**ğŸ”— Resources:**
- [PyTorch Conference 2024](https://www.infoq.com/news/2024/09/pytorch-conference-2024/)

</td>
<td width="50%">

### ğŸ’» **llama.cpp**
![llama.cpp](https://img.shields.io/badge/llama.cpp-CPU_Optimized-green?style=for-the-badge&logo=cplusplus)

> âš¡ CPU-optimized LLM inference

**Advantages:**
```diff
+ âœ… Lower memory usage
+ âœ… No GPU required
+ âœ… Fast generation
+ âœ… Cross-platform
+ âœ… Wide model support
```

**ğŸ”— Comparison:**
- [vLLM vs Ollama vs llama.cpp vs TGI vs TensorRT-LLM](https://itecsonline.com/post/vllm-vs-ollama-vs-llama.cpp-vs-tgi-vs-tensort)

</td>
</tr>
</table>

---

<div align="center">

## ğŸ”§ **Model Compression & Optimization**

![Compression](https://img.shields.io/badge/Model-Compression-FF6B6B?style=for-the-badge&logo=semanticscholar&logoColor=white)
![Quantization](https://img.shields.io/badge/Quantization-4bit__8bit-4ECDC4?style=for-the-badge&logo=hackthebox&logoColor=white)

</div>

---

### ğŸ“‰ **Advanced Quantization Techniques**

<table>
<tr>
<td width="33%">

#### ğŸ† **AWQ**
![Award](https://img.shields.io/badge/MLSys_2024-Best_Paper-gold?style=flat-square&logo=adobeacrobatreader)

**Activation-aware Weight Quantization**

> ğŸ¯ MIT HAN Lab Innovation

**Key Concept:**
```python
# Not all weights are equal!
if is_salient(weight):
    skip_quantization()
else:
    quantize_weight()
```

**Features:**
- âš¡ Protects critical weights
- ğŸ¯ Activation-aware
- ğŸ”¥ State-of-the-art results

**ğŸ”— Resources:**
- [AWQ GitHub](https://github.com/mit-han-lab/llm-awq)
- [MIT HAN Lab](https://hanlab.mit.edu/)

</td>
<td width="33%">

#### ğŸ’ **GPTQ**
![GPTQ](https://img.shields.io/badge/GPTQ-4bit-blue?style=flat-square&logo=lightning)

**GPU-Focused Quantization**

**Features:**
- ğŸ”¸ Row-wise quantization
- ğŸ”¸ Hessian optimization
- ğŸ”¸ GPU inference focused
- ğŸ”¸ 175B models supported

**Achievements:**
```yaml
Models: BLOOM, OPT-175B
Precision: 4-bit
Platform: GPU optimized
```

</td>
<td width="33%">

#### ğŸ”¬ **QLoRA**
![QLoRA](https://img.shields.io/badge/QLoRA-Fine--Tuning-purple?style=flat-square&logo=pytorch)

**Efficient Fine-tuning**

**Innovations:**
- âœ¨ 4-bit NormalFloat (NF4)
- âœ¨ Double quantization
- âœ¨ LoRA adapters
- âœ¨ Single GPU fine-tuning

**Capability:**
```diff
+ Fine-tune 65B model
+ On single GPU
+ Maintain quality
```

</td>
</tr>
</table>

---

#### ğŸ†• **Unsloth Dynamic 4-bit**
![Latest](https://img.shields.io/badge/Release-December_2024-brightgreen?style=for-the-badge&logo=github)

> ğŸ”¥ Latest quantization innovation

**Features:**
- Built on BitsandBytes
- Dynamic parameter quantization
- Per-parameter optimization

**ğŸ“š Comprehensive Guides:**
- ğŸ“– [Quantization Comparison](https://generativeai.pub/practical-guide-of-llm-quantization-gptq-awq-bitsandbytes-and-unsloth-bdeaa2c0bbf6)
- ğŸ“Š [GPTQ vs GGUF vs AWQ](https://newsletter.maartengrootendorst.com/p/which-quantization-method-is-right)

---

### ğŸ”¬ **Neural Architecture Search (NAS)**

<div align="center">

![NAS](https://img.shields.io/badge/Neural_Architecture-Search-blueviolet?style=for-the-badge&logo=pytorch)

</div>

> ğŸ¤– Automate neural network architecture design

#### ğŸ¯ **Once-for-All (OFA)**

**Concept:** Train once, deploy everywhere

```mermaid
graph TD
    A[ğŸŒ Supernet Training] --> B[ğŸ“¦ Weight Sharing]
    B --> C[ğŸ“± Mobile]
    B --> D[ğŸ’» Desktop]
    B --> E[âš¡ Edge]
    style A fill:#e1f5ff
    style B fill:#ffe1f5
    style C fill:#f5ffe1
    style D fill:#ffe1e1
    style E fill:#ffd700
```

**Features:**
- ğŸ”¹ Weight-sharing supernetwork
- ğŸ”¹ Represents any architecture in search space
- ğŸ”¹ Massive computational savings
- ğŸ”¹ Applied to ImageNet with ProxylessNAS & MobileNetV3

**ğŸ”— Resources:**
- [NAS Overview](https://www.automl.org/nas-overview/)
- [MIT HAN Lab NAS](https://hanlab.mit.edu/techniques/nas)

---

### ğŸ“ **Knowledge Distillation & Pruning**

<table>
<tr>
<td width="50%">

#### ğŸ”¬ **TinyBERT**
![TinyBERT](https://img.shields.io/badge/TinyBERT-7.5x_Smaller-success?style=for-the-badge&logo=semanticscholar)

> ğŸ“š Two-stage distillation approach

**Performance Metrics:**
```yaml
Accuracy: 96.8% of BERT-base
Size: 7.5x smaller (4 layers)
Energy: Lowest variability (0.1032 kWh SD)
Stages: Task-agnostic + Task-specific
```

**Advantages:**
- âœ… Dual-stage distillation
- âœ… Ultra-low energy variability
- âœ… Compact architecture
- âœ… High performance retention

</td>
<td width="50%">

#### ğŸ“– **DistilBERT**
![DistilBERT](https://img.shields.io/badge/DistilBERT-40%25_Smaller-blue?style=for-the-badge&logo=huggingface)

> âš¡ Single-phase task-agnostic distillation

**Performance Metrics:**
```yaml
Accuracy: 97% of BERT
Size Reduction: 40% smaller
Speed: 60% faster
Use Case: General-purpose
```

**Recent Research (2025):**
- ğŸ”¸ 32% energy reduction with pruning
- ğŸ”¸ Iterative distillation + adaptive pruning
- ğŸ”¸ Nature Scientific Reports

</td>
</tr>
</table>

**ğŸ“š Resources:**
- [Nature Scientific Reports 2025](https://www.nature.com/articles/s41598-025-07821-w)
- [DistilBERT Medium](https://medium.com/huggingface/distilbert-8cf3380435b5)

---

<div align="center">

## ğŸ¯ **TinyML & MCU-specific Advances**

![TinyML](https://img.shields.io/badge/TinyML-Microcontrollers-FF6B6B?style=for-the-badge&logo=arduino&logoColor=white)
![MIT](https://img.shields.io/badge/MIT-HAN_Lab-A31F34?style=for-the-badge&logo=mit&logoColor=white)

</div>

---

### ğŸ§  **MCUNet Series** - MIT HAN Lab

<table>
<tr>
<td width="33%">

#### ğŸ“± **MCUNetV1**
![V1](https://img.shields.io/badge/Version-1.0-blue?style=flat-square)

**Foundation:**
- ğŸ”¸ Neural architecture for MCUs
- ğŸ”¸ Co-designed model + inference engine
- ğŸ”¸ Ultra-low memory footprint

</td>
<td width="33%">

#### ğŸš€ **MCUNetV2**
![V2](https://img.shields.io/badge/Version-2.0-green?style=flat-square)

**Achievements:**
```yaml
ImageNet: 71.8% accuracy
Visual Wake: >90% (32kB SRAM)
Capability: Object detection
Platform: Tiny devices
```

</td>
<td width="33%">

#### âš¡ **MCUNetV3**
![V3](https://img.shields.io/badge/Version-3.0-orange?style=flat-square)

**Latest:**
- ğŸ”¸ Enhanced efficiency
- ğŸ”¸ State-of-the-art MCU AI
- ğŸ”¸ Production ready

</td>
</tr>
</table>

---

#### ğŸ“ **Additional MCU Tools**

<table>
<tr>
<td width="50%">

**ğŸ”§ TinyTL**
- Tiny transfer learning for MCUs
- On-device learning capabilities
- Minimal resource overhead

</td>
<td width="50%">

**âš™ï¸ PockEngine**
- Inference engine optimization
- MCU-specific acceleration
- Memory-efficient execution

</td>
</tr>
</table>

**ğŸ“š Resources:**
- ğŸŒ [MCUNet Official](https://mcunet.mit.edu/)
- ğŸ’» [MCUNet GitHub](https://github.com/mit-han-lab/mcunet)
- ğŸ“– [TinyML Projects](https://hanlab.mit.edu/projects/tinyml)

---

### ğŸ”¬ **TinyDL (Tiny Deep Learning)**

![TinyDL](https://img.shields.io/badge/TinyDL-2024-blueviolet?style=for-the-badge&logo=tensorflow)

> ğŸ¯ Evolution from TinyML to deep learning on edge

**Focus Areas:**
- ğŸ”¹ Deep learning on ultra-constrained hardware
- ğŸ”¹ Power consumption in **mW range**
- ğŸ”¹ On-device sensor analytics
- ğŸ”¹ Real-time inference

**ğŸ“„ Resources:**
- [TinyDL Survey](https://arxiv.org/html/2506.18927v1)

---

<div align="center">

## ğŸ”© **Hardware Acceleration & Platforms**

![Hardware](https://img.shields.io/badge/Hardware-Acceleration-gold?style=for-the-badge&logo=nvidia&logoColor=black)
![Edge](https://img.shields.io/badge/Edge-Devices-4ECDC4?style=for-the-badge&logo=raspberrypi&logoColor=white)

</div>

---

### ğŸ–¥ï¸ **Edge AI Platforms**

<table>
<tr>
<td width="50%">

#### ğŸŸ¢ **NVIDIA Jetson Orin Nano Super**
![NVIDIA](https://img.shields.io/badge/NVIDIA-Jetson-76B900?style=for-the-badge&logo=nvidia)

**Specifications:**
```yaml
Compute: 67 INT8 TOPS
Performance: 1.7x vs previous Orin
Price: $249
Release: Late 2024
Status: âœ… Available
```

**Features:**
- âš¡ Generative AI optimized
- ğŸ¯ Edge AI development kit
- ğŸ’° Affordable price point

</td>
<td width="50%">

#### ğŸ”· **Edge TPU & Neural Accelerators**

**Hardware Platforms:**

![Google](https://img.shields.io/badge/Google-Edge_TPU-4285F4?style=flat-square&logo=google)
- Google Pixel EdgeTPU
- Coral Dev Board

![Apple](https://img.shields.io/badge/Apple-Neural_Engine-000000?style=flat-square&logo=apple)
- Apple Neural Engine
- A-series chips

![Generic](https://img.shields.io/badge/Generic-AI_Accelerators-orange?style=flat-square&logo=sparkfun)
- Specialized NPUs
- Custom ASICs

</td>
</tr>
</table>

---

### ğŸ“± **Mobile Deployment Targets**

<div align="center">

| Platform | Architecture | Use Case |
|----------|-------------|----------|
| ğŸ”§ **ARM CPUs** | ARM Cortex | General compute |
| ğŸ“¡ **Mobile DSPs** | Qualcomm/MediaTek | Signal processing |
| ğŸ® **Mobile GPUs** | Mali/Adreno | Graphics + AI |
| ğŸ§  **NPUs** | Custom ASICs | Neural processing |

</div>

---

<div align="center">

## ğŸ› ï¸ **Implementation Resources & Tools**

![ONNX](https://img.shields.io/badge/ONNX-Runtime-blue?style=for-the-badge&logo=onnx&logoColor=white)
![TensorRT](https://img.shields.io/badge/TensorRT-NVIDIA-76B900?style=for-the-badge&logo=nvidia&logoColor=white)

</div>

---

### ğŸ”· **ONNX Runtime**

> Cross-platform inference with ONNX models

<table>
<tr>
<td width="50%" valign="top">

#### ğŸ“š **Documentation & Tutorials**
- ğŸ“– [ONNX Runtime C++ Inference](https://leimao.github.io/blog/ONNX-Runtime-CPP-Inference/)
- ğŸ [PyTorch to ONNX Tutorial](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)
- ğŸ“ [ONNX Registry Tutorial](https://pytorch.org/tutorials/beginner/onnx/onnx_registry_tutorial.html)
- ğŸ“ [On-Device Training](https://onnxruntime.ai/docs/api/python/on_device_training/training_artifacts.html)

#### ğŸ”§ **Compatibility**
- âš™ï¸ [ONNX Runtime Compatibility](https://onnxruntime.ai/docs/reference/compatibility.html)
- ğŸ“‹ [ONNX Versioning](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)
- ğŸš€ [CUDA Execution Provider](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html)

</td>
<td width="50%" valign="top">

#### ğŸ’» **Example Implementations**
- ğŸ–¼ï¸ [C++ ResNet Console App](https://github.com/cassiebreviu/cpp-onnxruntime-resnet-console-app)
- âš¡ [ONNX Runtime C++ Example](https://github.com/k2-gc/onnxruntime-cpp-example)
- ğŸ¤– [ONNX Runtime Android](https://github.com/Rohithkvsp/OnnxRuntimeAndorid)
- ğŸ¯ [ByteTrack ONNX Inference](https://github.com/ifzhang/ByteTrack/blob/main/deploy/ONNXRuntime/onnx_inference.py)

#### ğŸ“¦ **Model Repositories**
- ğŸ¤— [HuggingFace ONNX Models](https://huggingface.co/models?sort=trending&search=onnx)
- ğŸ”§ [txtai ONNX Pipeline](https://neuml.github.io/txtai/pipeline/train/hfonnx/)
- ğŸ“¤ [Ultralytics Export](https://docs.ultralytics.com/modes/export/#arguments)

</td>
</tr>
</table>

---

### ğŸ“‰ **ONNX Runtime Quantization**

![Quantization](https://img.shields.io/badge/Quantization-INT8_FP16-success?style=for-the-badge&logo=semanticscholar)

**Tools & Resources:**
- ğŸ”§ [Quantization Tools](https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/quantization)
- ğŸ“Š [Float16 Optimization](https://onnxruntime.ai/docs/performance/model-optimizations/float16.html)
- ğŸ’¡ [Quantization Examples](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization)

---

### ğŸ¯ **YOLO Implementations**

<details>
<summary><b>ğŸ”¥ Click to expand YOLO implementations</b></summary>

<br>

#### ğŸŸ£ **YOLO-NAS with ONNX**
- ğŸ’» [YOLO-NAS ONNXRuntime](https://github.com/jason-li-831202/YOLO-NAS-onnxruntime)

#### ğŸŸ¢ **YOLO + TensorRT** (Detection, Pose, Segmentation)
- âš¡ [YOLOv8-TensorRT-CPP](https://github.com/cyrusbehr/YOLOv8-TensorRT-CPP)
- ğŸ”§ [TensorRT C++ API](https://github.com/cyrusbehr/tensorrt-cpp-api)
- ğŸ [YOLOv8-TensorRT (Python + C++)](https://github.com/triple-Mu/YOLOv8-TensorRT)
- ğŸ¤¸ [YOLO Pose C++](https://github.com/mattiasbax/yolo-pose_cpp)
- ğŸ“š [TensorRT Samples](https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec)
- ğŸ“º [YOLOv8 TensorRT Tutorial](https://www.youtube.com/watch?v=Z0n5aLmcRHQ)

#### ğŸ”µ **YOLO + ONNXRuntime** (All Tasks)
- ğŸ’» [YOLOv8-ONNX-CPP](https://github.com/FourierMourier/yolov8-onnx-cpp/tree/main)
- ğŸ¤¸ [YOLOv8 Pose Implementation](https://github.com/mallumoSK/yolov8/blob/master/yolo/YoloPose.cpp)
- âš¡ [YOLOv8 TensorRT Pose](https://github.com/triple-Mu/YOLOv8-TensorRT/blob/main/csrc/pose/normal/main.cpp)
- ğŸ”§ [YOLO-ONNXRuntime-CPP](https://github.com/Amyheart/yolo-onnxruntime-cpp)
- ğŸ“· [YOLOv8-OpenCV-ONNXRuntime-CPP](https://github.com/UNeedCryDear/yolov8-opencv-onnxruntime-cpp)
- ğŸ“– [Ultralytics YOLOv8 C++](https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-ONNXRuntime-CPP)
- ğŸ¯ [YOLOv6-OpenCV-ONNXRuntime](https://github.com/hpc203/yolov6-opencv-onnxruntime/tree/main)
- ğŸƒ [YOLOv5 Pose OpenCV](https://github.com/hpc203/yolov5_pose_opencv)

#### ğŸŒ **Community Resources**
- ğŸ‘¨â€ğŸ’» [hpc203 Repositories](https://github.com/hpc203?tab=repositories)
- ğŸ’¬ [YOLO Issue Discussions](https://github.com/ultralytics/ultralytics/issues/1852)
- ğŸ› [YOLOv5 Fixed Bugs](https://github.com/ultralytics/yolov5/issues/916)
- ğŸ‡¨ğŸ‡³ [Chinese Tutorial](https://zhuanlan.zhihu.com/p/466677699)
- ğŸ“¦ [ONNX Runtime Install Guide](https://velog.io/@dnchoi/ONNX-runtime-install)

</details>

---

### âš¡ **TensorRT**

![TensorRT](https://img.shields.io/badge/TensorRT-Inference_Optimizer-76B900?style=for-the-badge&logo=nvidia)

> ğŸš€ NVIDIA's high-performance deep learning inference optimizer

**Resources:**
- ğŸ”§ [TensorRT Execution Provider](https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements)
- ğŸ’¾ [TensorRT Engine Cache](https://gitee.com/arnoldfychen/onnxruntime/blob/master/docs/execution_providers/TensorRT-ExecutionProvider.md#specify-tensorrt-engine-cache-path)

---

<div align="center">

## ğŸŒ **Edge Deployment Frameworks**

![Deployment](https://img.shields.io/badge/Edge-Deployment-FF6B6B?style=for-the-badge&logo=kubernetes&logoColor=white)
![Frameworks](https://img.shields.io/badge/Frameworks-Multi--Platform-4ECDC4?style=for-the-badge&logo=docker&logoColor=white)

</div>

---

<table>
<tr>
<td width="50%">

### ğŸš€ **FastDeploy** - PaddlePaddle
![PaddlePaddle](https://img.shields.io/badge/PaddlePaddle-FastDeploy-blue?style=for-the-badge)

> ğŸ“¦ Easy-to-use deployment toolbox for AI models

**Resources:**
- ğŸ’» [FastDeploy GitHub](https://github.com/PaddlePaddle/FastDeploy)
- ğŸ“¥ [Prebuilt Libraries](https://github.com/PaddlePaddle/FastDeploy/blob/develop/docs/en/build_and_install/download_prebuilt_libraries.md)

---

### ğŸ’ **DeepSparse & SparseML** - Neural Magic
![Neural Magic](https://img.shields.io/badge/Neural_Magic-DeepSparse-purple?style=for-the-badge)

> ğŸ–¥ï¸ CPU-optimized inference with sparsity

**Features:**
- âš¡ CPU inference acceleration
- ğŸ”¸ Sparsity-aware optimization
- ğŸ“Š YOLOv5 CPU benchmarks

**Resources:**
- ğŸ“ˆ [YOLOv5 CPU Benchmark](https://neuralmagic.com/blog/benchmark-yolov5-on-cpus-with-deepsparse/)
- ğŸ’» [SparseML GitHub](https://github.com/neuralmagic/sparseml/tree/main)
- ğŸš€ [DeepSparse GitHub](https://github.com/neuralmagic/deepsparse)

</td>
<td width="50%">

### ğŸ“± **NCNN** - Tencent
![Tencent](https://img.shields.io/badge/Tencent-NCNN-00D9FF?style=for-the-badge)

> ğŸ¯ High-performance neural network inference for mobile

**Resources:**
- ğŸ’» [NCNN GitHub](https://github.com/Tencent/ncnn)
- ğŸ“– [NCNN C++ Usage](https://github.com/Tencent/ncnn/blob/master/docs/how-to-use-and-FAQ/use-ncnn-with-alexnet.md)
- ğŸ“± [YoloMobile](https://github.com/wkt/YoloMobile)
- â­ [Awesome NCNN](https://github.com/umitkacar/awesome-ncnn-collection)
- ğŸ”„ [Model Converter](https://convertmodel.com/)

---

### ğŸ”§ **MACE** - Xiaomi
![Xiaomi](https://img.shields.io/badge/Xiaomi-MACE-FF6900?style=for-the-badge)

> ğŸ¤– Mobile AI Compute Engine

**Resources:**
- ğŸ’» [MACE GitHub](https://github.com/xiaomi/mace)

</td>
</tr>
</table>

---

### ğŸ **CoreML** - Apple

![Apple](https://img.shields.io/badge/Apple-CoreML-000000?style=for-the-badge&logo=apple)

> ğŸ¨ Machine learning framework for iOS/macOS

<details>
<summary><b>ğŸ“¦ Click to expand CoreML resources</b></summary>

<br>

#### ğŸ¨ **Model Collections**
- ğŸ¯ [Semantic Segmentation CoreML](https://github.com/tucan9389/SemanticSegmentation-CoreML)
- ğŸ“š [CoreML Models Collection](https://github.com/john-rocky/CoreML-Models#u2net)
- â­ [Awesome CoreML Models](https://github.com/likedan/Awesome-CoreML-Models)
- ğŸ§  [Awesome CoreML Models 2](https://github.com/SwiftBrain/awesome-CoreML-models)
- ğŸ¬ [RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting)

#### ğŸ› ï¸ **Tools & Documentation**
- ğŸ”„ [PyTorch to CoreML](https://coremltools.readme.io/docs/pytorch-conversion)
- ğŸ”§ [CoreML Helpers](https://github.com/hollance/CoreMLHelpers)
- ğŸ“– [Apple ML API](https://developer.apple.com/machine-learning/api/)
- ğŸ“Š [CoreML Performance Tool](https://github.com/vladimir-chernykh/coreml-performance)

#### ğŸ¨ **Stable Diffusion on CoreML**
- ğŸ”¬ [Apple ML-4M](https://github.com/apple/ml-4m/)
- ğŸ¯ [Apple ML Stable Diffusion](https://github.com/apple/ml-stable-diffusion)
- ğŸ“¦ [Stable Diffusion 2 Base](https://huggingface.co/stabilityai/stable-diffusion-2-base)
- ğŸš€ [Stability AI SD](https://github.com/Stability-AI/stablediffusion)
- ğŸ“š [Stable Diffusion v1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4)
- ğŸ¬ [RunwayML SD](https://github.com/runwayml/stable-diffusion)
- ğŸ–¼ï¸ [Automatic1111 WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

</details>

---

<div align="center">

## âš™ï¸ **Compilers & Low-Level Frameworks**

![Compilers](https://img.shields.io/badge/Compilers-Low--Level-blueviolet?style=for-the-badge&logo=llvm&logoColor=white)
![Optimization](https://img.shields.io/badge/Hardware-Optimization-gold?style=for-the-badge&logo=arm&logoColor=black)

</div>

---

<table>
<tr>
<td width="50%">

### ğŸ”§ **TVM** - Apache
![TVM](https://img.shields.io/badge/Apache-TVM-D22128?style=for-the-badge&logo=apache)

> ğŸ¯ End-to-end deep learning compiler stack

**Resources:**
- [TVM GitHub](https://github.com/apache/tvm)

---

### ğŸ”¨ **LLVM**
![LLVM](https://img.shields.io/badge/LLVM-Compiler-262D3A?style=for-the-badge&logo=llvm)

> âš™ï¸ Compiler infrastructure project

**Resources:**
- [LLVM Project](https://github.com/llvm/llvm-project)

---

### âš¡ **XNNPack** - Google
![Google](https://img.shields.io/badge/Google-XNNPack-4285F4?style=for-the-badge&logo=google)

> ğŸš€ High-efficiency floating-point neural network operators

**Resources:**
- [XNNPack GitHub](https://github.com/google/XNNPACK)

</td>
<td width="50%">

### ğŸ”· **ARM-NN**
![ARM](https://img.shields.io/badge/ARM-NN-0091BD?style=for-the-badge&logo=arm)

> ğŸ’ª Inference engine for ARM platforms

**Resources:**
- [ARM-NN GitHub](https://github.com/ARM-software/armnn)
- [ARM-NN Tutorial](https://www.youtube.com/watch?v=QuNOaFLobSg)

---

### ğŸ§  **CMSIS-NN**
![CMSIS](https://img.shields.io/badge/ARM-CMSIS--NN-00979D?style=for-the-badge&logo=arm)

> ğŸ“± Efficient neural network kernels for ARM Cortex-M

**Resources:**
- [CMSIS-NN GitHub](https://github.com/ARM-software/CMSIS_5)

---

### ğŸ“± **Samsung ONE**
![Samsung](https://img.shields.io/badge/Samsung-ONE-1428A0?style=for-the-badge&logo=samsung)

> ğŸ”§ On-device Neural Engine compiler

**Resources:**
- [ONE GitHub](https://github.com/Samsung/ONE)

</td>
</tr>
</table>

---

<div align="center">

## ğŸ’¼ **Industry & Commercial Solutions**

![Industry](https://img.shields.io/badge/Industry-Solutions-FF6B6B?style=for-the-badge&logo=enterprisedb&logoColor=white)

</div>

---

### ğŸš€ **Deeplite**

![Deeplite](https://img.shields.io/badge/Deeplite-AI_Optimizer-4ECDC4?style=for-the-badge)

> ğŸ¯ AI-Driven Optimizer for Deep Neural Networks

**Focus:**

<table>
<tr>
<td width="20%" align="center">âš¡<br><b>Faster<br>Inference</b></td>
<td width="20%" align="center">ğŸ“¦<br><b>Smaller<br>Models</b></td>
<td width="20%" align="center">ğŸ”‹<br><b>Energy<br>Efficient</b></td>
<td width="20%" align="center">â˜ï¸<br><b>Cloud to<br>Edge</b></td>
<td width="20%" align="center">ğŸ¯<br><b>Maintain<br>Accuracy</b></td>
</tr>
</table>

**ğŸ”— Resources:**
- [Deeplite Website](https://www.deeplite.ai/)

---

<div align="center">

## ğŸ”§ **Utility Frameworks & Tools**

![Tools](https://img.shields.io/badge/Utility-Tools-00D9FF?style=for-the-badge&logo=hackthebox&logoColor=white)

</div>

---

<table>
<tr>
<td width="50%">

### ğŸ‘ï¸ **OpenCV**
![OpenCV](https://img.shields.io/badge/OpenCV-Computer_Vision-5C3EE8?style=for-the-badge&logo=opencv)

> ğŸ“· Computer vision library with C++ support

**Resources:**
- ğŸ“º [OpenCV C++ Playlist](https://www.youtube.com/playlist?list=PLUTbi0GOQwghR9db9p6yHqwvzc989q_mu)
- ğŸ”¨ [Build OpenCV C++](https://gist.github.com/raulqf/f42c718a658cddc16f9df07ecc627be7)

</td>
<td width="50%">

### ğŸ¬ **VQRF** - Video Compression
![VQRF](https://img.shields.io/badge/VQRF-Video_Compression-red?style=for-the-badge&logo=youtube)

> ğŸ“¹ Vector Quantized Radiance Fields

**Resources:**
- [VQRF GitHub](https://github.com/AlgoHunt/VQRF)

</td>
</tr>
</table>

---

<div align="center">

## ğŸ–¼ï¸ **Additional Model Architectures**

![Models](https://img.shields.io/badge/Model-Architectures-blueviolet?style=for-the-badge&logo=pytorch&logoColor=white)

</div>

---

<table>
<tr>
<td width="50%">

### ğŸ¯ **PP-PicoDet**
![PicoDet](https://img.shields.io/badge/PaddlePaddle-PicoDet-blue?style=for-the-badge)

> ğŸ“± Lightweight real-time object detector for mobile

**Resources:**
- [PP-PicoDet Paper](https://arxiv.org/pdf/2111.00902.pdf)

</td>
<td width="50%">

### ğŸ”¬ **EtinyNet**
![EtinyNet](https://img.shields.io/badge/EtinyNet-TinyML-orange?style=for-the-badge&logo=arduino)

> ğŸ¯ Extremely tiny network for TinyML

**Resources:**
- [EtinyNet GitHub](https://github.com/aztc/EtinyNet)

</td>
</tr>
</table>

<div align="center">

![TinyML Architecture](./tinyML.png)

</div>

---

<div align="center">

## ğŸ§  **Computing Architectures & APIs**

![Computing](https://img.shields.io/badge/Computing-Architectures-gold?style=for-the-badge&logo=nvidia&logoColor=black)

</div>

---

<table align="center">
<tr>
<td align="center" width="16.66%">

![ARM](https://img.shields.io/badge/ARM-0091BD?style=for-the-badge&logo=arm&logoColor=white)

**Mobile &<br>Embedded**

</td>
<td align="center" width="16.66%">

![RISC-V](https://img.shields.io/badge/RISC--V-283272?style=for-the-badge&logo=riscv&logoColor=white)

**Open-Source<br>ISA**

</td>
<td align="center" width="16.66%">

![CUDA](https://img.shields.io/badge/CUDA-76B900?style=for-the-badge&logo=nvidia&logoColor=white)

**NVIDIA<br>GPU**

</td>
<td align="center" width="16.66%">

![Metal](https://img.shields.io/badge/Metal-000000?style=for-the-badge&logo=apple&logoColor=white)

**Apple<br>GPU**

</td>
<td align="center" width="16.66%">

![OpenCL](https://img.shields.io/badge/OpenCL-721412?style=for-the-badge&logo=opencl&logoColor=white)

**Cross-<br>Platform**

</td>
<td align="center" width="16.66%">

![Vulkan](https://img.shields.io/badge/Vulkan-AC162C?style=for-the-badge&logo=vulkan&logoColor=white)

**Graphics &<br>Compute**

</td>
</tr>
</table>

---

<div align="center">

## ğŸ“š **Research Papers & Academic Resources**

![Research](https://img.shields.io/badge/Research-Papers-FF6B6B?style=for-the-badge&logo=semanticscholar&logoColor=white)
![2024-2025](https://img.shields.io/badge/Years-2024--2025-4ECDC4?style=for-the-badge&logo=academiasquare&logoColor=white)

</div>

---

### ğŸ“– **Foundational Surveys (2024-2025)**

<details open>
<summary><b>ğŸ” Click to expand research papers</b></summary>

<br>

#### ğŸŒ **Edge Computing & Deep Learning**
- ğŸ“„ [Deep Learning With Edge Computing: A Review](https://www.cs.ucr.edu/~jiasi/pub/deep_edge_review.pdf)
- ğŸ“„ [Convergence of Edge Computing and Deep Learning](https://arxiv.org/pdf/1907.08349.pdf)
- ğŸ“„ [Machine Learning at the Network Edge](https://arxiv.org/pdf/1908.00080.pdf)
- ğŸ“„ [Edge Deep Learning in CV & Medical Diagnostics](https://link.springer.com/article/10.1007/s10462-024-11033-5)

#### ğŸ”¬ **TinyML Specific**
- ğŸ“„ [From Tiny ML to Tiny DL: A Survey (2024)](https://arxiv.org/html/2506.18927v1)
- ğŸ“„ [EtinyNet: Extremely Tiny Network](https://ojs.aaai.org/index.php/AAAI/article/download/20387/version/18684/20146)
- ğŸ“„ [Ultra-low Power TinyML System](https://arxiv.org/pdf/2207.04663.pdf)

#### âš¡ **State Space Models & Efficient Architectures**
- ğŸ“„ [Mamba: Linear-Time Sequence Modeling](https://arxiv.org/abs/2312.00752)
- ğŸ“„ [Mamba-360: Survey of SSMs](https://arxiv.org/html/2404.16112v1)
- ğŸ“„ [eMamba: Efficient Edge Acceleration](https://arxiv.org/html/2508.10370)

#### ğŸ‘ï¸ **Vision Models**
- ğŸ“„ [MobileNetV4 (ECCV 2024)](https://link.springer.com/chapter/10.1007/978-3-031-73661-2_5)
- ğŸ“„ [ViT for Mobile/Edge Devices](https://link.springer.com/article/10.1007/s00530-024-01312-0)
- ğŸ“„ [YOLO Evolution: v5 to YOLO26](https://arxiv.org/html/2510.09653v2)
- ğŸ“„ [YOLOv10: Real-Time Detection](https://arxiv.org/pdf/2405.14458)

#### ğŸ”§ **Model Compression & Optimization**
- ğŸ“„ [Model Compression for Carbon Efficient AI (2025)](https://www.nature.com/articles/s41598-025-07821-w)
- ğŸ“„ [NAS Systematic Review (2024)](https://link.springer.com/article/10.1007/s10462-024-11058-w)
- ğŸ“„ [Advances in Neural Architecture Search](https://academic.oup.com/nsr/article/11/8/nwae282/7740455)

#### ğŸ“š **Collections**
- â­ [Awesome Embedded and Mobile Deep Learning](https://github.com/csarron/awesome-emdl/blob/master/README.md)

</details>

---

<div align="center">

## ğŸ“ **Contributing & Community**

![Community](https://img.shields.io/badge/Community-Welcome-success?style=for-the-badge&logo=github&logoColor=white)
![Contributions](https://img.shields.io/badge/Contributions-Open-blue?style=for-the-badge&logo=githubactions&logoColor=white)

</div>

---

<div align="center">

This repository serves as a **comprehensive resource** for AI edge computing and TinyML practitioners.

**Contributions, updates, and corrections are welcome!** ğŸš€

---

### ğŸ“Š **Repository Stats**

![Last Commit](https://img.shields.io/github/last-commit/umitkacar/ai-edge-computing-tiny-embedded?style=for-the-badge)
![Contributors](https://img.shields.io/github/contributors/umitkacar/ai-edge-computing-tiny-embedded?style=for-the-badge)
![Issues](https://img.shields.io/github/issues/umitkacar/ai-edge-computing-tiny-embedded?style=for-the-badge)

---

### ğŸ·ï¸ **Keywords**

`TinyML` â€¢ `Edge AI` â€¢ `Embedded ML` â€¢ `Model Compression` â€¢ `Quantization` â€¢ `Neural Architecture Search` â€¢ `YOLO` â€¢ `MobileNet` â€¢ `Transformer` â€¢ `State Space Models` â€¢ `ONNX Runtime` â€¢ `TensorRT` â€¢ `Inference Optimization` â€¢ `MCU` â€¢ `IoT` â€¢ `Real-Time AI`

---

### ğŸ“… **Last Updated**
**January 2025**

---

<img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=150&section=footer&text=Thank%20You!&fontSize=42&fontColor=fff&animation=twinkling&fontAlignY=72"/>

</div>
